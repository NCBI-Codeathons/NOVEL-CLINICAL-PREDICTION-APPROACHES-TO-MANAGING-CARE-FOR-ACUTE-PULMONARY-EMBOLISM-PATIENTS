{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to files\n",
    "data_path = 'Q:/hackers09/shared/data/'\n",
    "cancer_path = os.path.join(data_path, 'df_cancer.csv')\n",
    "echo_path = os.path.join(data_path, 'df_echo.csv')\n",
    "encounter_path = os.path.join(data_path, 'df_encounter.csv')\n",
    "labs_path = os.path.join(data_path, 'df_labs.csv')\n",
    "outcome_path = os.path.join(data_path, 'df_outcome.csv')\n",
    "problist_path = os.path.join(data_path, 'df_problist.csv')\n",
    "radiology_path = os.path.join(data_path, 'df_radiology.csv')\n",
    "registry_path = os.path.join(data_path, 'df_registry.csv')\n",
    "vitals_path = os.path.join(data_path, 'df_vitals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataframes\n",
    "cancer_df = pd.read_csv(cancer_path, encoding='ISO-8859-1')\n",
    "cancer_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "echo_df = pd.read_csv(echo_path, encoding='ISO-8859-1')\n",
    "echo_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "encounter_df = pd.read_csv(encounter_path, encoding='ISO-8859-1')\n",
    "encounter_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "labs_df = pd.read_csv(labs_path, encoding='ISO-8859-1')\n",
    "labs_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "outcome_df = pd.read_csv(outcome_path, encoding='ISO-8859-1')\n",
    "outcome_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "problist_df = pd.read_csv(problist_path, encoding='ISO-8859-1')\n",
    "problist_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "radiology_df = pd.read_csv(radiology_path, encoding='ISO-8859-1')\n",
    "radiology_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "registry_df = pd.read_csv(registry_path, encoding='ISO-8859-1')\n",
    "registry_df.set_index(\"HSP_ENC\", inplace = True)\n",
    "vitals_df = pd.read_csv(vitals_path, encoding='ISO-8859-1')\n",
    "vitals_df.set_index(\"HSP_ENC\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Define functions to merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to merge cancer data\n",
    "def merge_cancer(enc_df, cnc_df):\n",
    "    #Make modifications to cancer df\n",
    "    mod_cancer_df = cnc_df.copy()\n",
    "    mod_cancer_df.reset_index(level=0, inplace=True)\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Unknown, Missing Remission Date', 'cancer_at_enc'] = 'Unknown'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Unknown, Previously Positive', 'cancer_at_enc'] = 'Unknown'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Unknown, Not Documented', 'cancer_at_enc'] = 'Unknown'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Unknown, Not documented', 'cancer_at_enc'] = 'Unknown'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'No Cancer', 'cancer_at_enc'] = '1'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Unknown', 'cancer_at_enc'] = '2'\n",
    "    mod_cancer_df.loc[mod_cancer_df.cancer_at_enc == 'Cancer', 'cancer_at_enc'] = '3'\n",
    "    mod_cancer_df['cancer_at_enc'] = mod_cancer_df['cancer_at_enc'].astype(int)\n",
    "    mod_cancer_df.drop_duplicates(['PATIENT_ID', 'HSP_ENC'])\n",
    "    \n",
    "    #Take only cancer status column with max value\n",
    "    drop_cancer_df = mod_cancer_df[['HSP_ENC', 'cancer_at_enc']]\n",
    "    drop_cancer_df = drop_cancer_df.groupby('HSP_ENC',group_keys=False).apply(lambda x: x.loc[x['cancer_at_enc']==x['cancer_at_enc'].max()])\n",
    "\n",
    "    #Merge with encounter df and drop dups\n",
    "    mergeRes = pd.merge(enc_df, drop_cancer_df, on='HSP_ENC', how='left')\n",
    "    mergeRes = mergeRes.drop_duplicates('HSP_ENC')\n",
    "\n",
    "    #Replace NaN in encounter df with 0 (Never had cancer)\n",
    "    mergeRes['cancer_at_enc'].fillna(0, inplace=True)\n",
    "    \n",
    "    #Renaming the cancer column\n",
    "    mergeRes.rename(columns={'cancer_at_enc': 'CANCER_RANK'}, inplace=True)\n",
    "    \n",
    "    mergeRes.reset_index(drop=True)\n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_registry(enc_df, reg_df):\n",
    "    reg_list = reg_df.REGISTRY_NAME.unique().tolist()\n",
    "    reg_df_copy = reg_df.copy()\n",
    "    reg_df_copy.reset_index(level=0, inplace=True)\n",
    "\n",
    "    tst_df = reg_df_copy[['HSP_ENC']].copy()\n",
    "    for item in reg_list:\n",
    "        tst_df[item] = False\n",
    "    tst_df = tst_df.drop_duplicates()\n",
    "\n",
    "\n",
    "    for index, row in reg_df_copy.iterrows():\n",
    "        enc_id = reg_df_copy.iloc[index, 0]\n",
    "        curr_reg = reg_df_copy.iloc[index, 2]\n",
    "        tst_df.loc[tst_df['HSP_ENC'] == enc_id, [curr_reg]] = True\n",
    "\n",
    "    #Merge with encounter df and drop dups\n",
    "    mergeRes = pd.merge(enc_df, tst_df, on='HSP_ENC', how='left')\n",
    "    mergeRes = mergeRes.drop_duplicates('HSP_ENC')\n",
    "    \n",
    "    #Replace NaN in encounter df with False, no record\n",
    "    for item in reg_list:\n",
    "        mergeRes[item].fillna(False, inplace=True)\n",
    "    \n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vitals(enc_df, data_path):\n",
    "    vital_data_path = os.path.join(data_path, 'vitals.csv')\n",
    "    vital_data_df = pd.read_csv(vital_data_path, encoding='ISO-8859-1')\n",
    "    vital_data_df = vital_data_df.drop('Unnamed: 0', 1)\n",
    "    \n",
    "    mergeRes = pd.merge(enc_df, vital_data_df, on='HSP_ENC', how='left')\n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_co_morbid(enc_df, data_path):\n",
    "    co_mobid_path = os.path.join(data_path, 'comorbitidity_score_sm2.csv')\n",
    "    co_mobid_df = pd.read_csv(co_mobid_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    mergeRes = pd.merge(enc_df, co_mobid_df, on='HSP_ENC', how='left')\n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labs(enc_df, data_path):\n",
    "    lab_data_path = os.path.join(data_path, 'cleaned_condensed_lab_data.csv')\n",
    "    lab_data_df = pd.read_csv(lab_data_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    mergeRes = pd.merge(enc_df, lab_data_df, on='HSP_ENC', how='left')\n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiology / CT / Echo / Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_encounter_radiology(df_encounter, df_radiology, cutoff_OrderTime=12, cutoff_ED_Disp=12):\n",
    "    # generate a new df_CT dataframe\n",
    "    # 1. focus on 'CT ANGIOGRAM' only\n",
    "    # 2. order time within 12h\n",
    "    # 3. keep only the first order for the outliers (only one data point that has 2 orders)\n",
    "\n",
    "    df_CT = df_radiology[df_radiology['NAME'].apply(lambda x: x.startswith('CT AN'))]\n",
    "    df_CT = df_CT[df_CT['ORDER_TIME_DIFFSEC'] <= cutoff_OrderTime*3600]\n",
    "    df_CT = df_CT.drop_duplicates('HSP_ENC', keep='first')\n",
    "\n",
    "    # combine df_encounter and df_CT based on 'HSP_ENC' id\n",
    "    df_encounter = df_encounter[df_encounter['ED_DISP_TIME_DIFFSEC']<=cutoff_ED_Disp*3600]\n",
    "    df_enc_CT = pd.merge(df_encounter, df_CT, how='left', on='HSP_ENC')\n",
    "    df_enc_CT.set_index('HSP_ENC', inplace=True)\n",
    "    \n",
    "    return df_enc_CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_outcome(dfOutcome):\n",
    "    ## Outcome expansion   \n",
    "    \n",
    "    # Drop any outcomes after 48 hr\n",
    "    dfOutcome = dfOutcome[dfOutcome['ORDER_TIME_DIFFSEC']<48*60*60]\n",
    "    \n",
    "    # Create columns for each unique outcome\n",
    "    lsExpandedOutcomeCols = []\n",
    "    lsOutcomeColsToRetain = dfOutcome.columns[2:].tolist()\n",
    "    lsUniqueOutcomes = dfOutcome['name_gen'].value_counts().index.tolist()\n",
    "    for strOutcome in lsUniqueOutcomes:\n",
    "        lsExpandedOutcomeCols += [strCol + '_' + strOutcome for strCol in lsOutcomeColsToRetain]\n",
    "\n",
    "    dfExpandedOutcome = pd.DataFrame(columns=lsExpandedOutcomeCols+['PATIENT_ID'])\n",
    "    dfExpandedOutcome['HSP_ENC'] = dfOutcome['HSP_ENC'].value_counts().index\n",
    "    dfExpandedOutcome = dfExpandedOutcome.set_index('HSP_ENC')\n",
    "\n",
    "    for nEnc in dfOutcome['HSP_ENC'].value_counts().index:\n",
    "        for nIdx in dfOutcome[dfOutcome['HSP_ENC']==nEnc].index:\n",
    "            lsTempCols = [strCol + '_' + dfOutcome.at[nIdx, 'name_gen'] for strCol in lsOutcomeColsToRetain]\n",
    "            lsTempCols.append('PATIENT_ID')\n",
    "            dfExpandedOutcome.loc[nEnc, lsTempCols] = dfOutcome.loc[nIdx, lsOutcomeColsToRetain+['PATIENT_ID']].values\n",
    "\n",
    "    # Create boolean column for order time < 48 hrs for any outcome\n",
    "    lsOrderTimeCols = [strCol for strCol in dfExpandedOutcome.columns if 'ORDER_TIME' in strCol]\n",
    "\n",
    "    dfExpandedOutcome['b48hr'] = np.zeros(dfExpandedOutcome.shape[0])\n",
    "    for nEnc in dfExpandedOutcome.index:\n",
    "        for nVal in dfExpandedOutcome.loc[nEnc, lsOrderTimeCols].values:\n",
    "            if nVal < 172800:\n",
    "                dfExpandedOutcome.at[nEnc, 'b48hr'] = 1\n",
    "\n",
    "    # Column for minimum order time from all outcomes\n",
    "    dfExpandedOutcome['MinOrderTime'] = dfExpandedOutcome.loc[:, lsOrderTimeCols].min(axis=1)\n",
    "    dfExpandedOutcome = dfExpandedOutcome.sort_values('MinOrderTime')\n",
    "    \n",
    "    return dfExpandedOutcome\n",
    "\n",
    "\n",
    "def compile_echo(dfEcho):    \n",
    "    # Keep only 12 hr echos\n",
    "    dfEcho = dfEcho[dfEcho['ORDER_INST_DIFFSEC'] < 12*60*60]\n",
    "\n",
    "    # Add all narratives together\n",
    "    lsUniqueEchoEnc = dfEcho['HSP_ENC'].value_counts().index.tolist()\n",
    "\n",
    "    lsEchoCols = dfEcho.columns.tolist()\n",
    "    lsEchoCols.remove('new_line')\n",
    "    lsEchoCols.remove('NARRATIVE')\n",
    "    dfCompiledEcho = pd.DataFrame(columns=lsEchoCols)\n",
    "    dfCompiledEcho['HSP_ENC'] = lsUniqueEchoEnc\n",
    "    dfCompiledEcho = dfCompiledEcho.set_index('HSP_ENC')\n",
    "    lsEchoCols.remove('HSP_ENC')\n",
    "\n",
    "    lsCompiledNarratives = []\n",
    "    for nEnc in lsUniqueEchoEnc:\n",
    "        lsUniqueEchoOrderId = dfEcho[dfEcho['HSP_ENC']==nEnc]['ORDER_PROC_ID'].value_counts().index.tolist()\n",
    "        #if len(lsUniqueEchoOrderId)>1:\n",
    "            #print(nEnc)\n",
    "        nFirstEchoOrderId = lsUniqueEchoOrderId[0] # Keep first one only\n",
    "        \n",
    "        # Compile echo data\n",
    "        strCompiled = dfEcho[dfEcho['ORDER_PROC_ID']==nFirstEchoOrderId]['NARRATIVE'].str.cat(sep=' ')\n",
    "        dfCompiledEcho.loc[nEnc, lsEchoCols] = dfEcho[dfEcho['ORDER_PROC_ID']==nFirstEchoOrderId][lsEchoCols].iloc[0]\n",
    "        lsCompiledNarratives.append(strCompiled)\n",
    "    dfCompiledEcho['NARRATIVE_compiled'] = lsCompiledNarratives\n",
    "\n",
    "    # Keep only 12 hr echos\n",
    "    dfCompiledEcho = dfCompiledEcho[dfCompiledEcho['ORDER_INST_DIFFSEC'] < 12*60*60]\n",
    "    \n",
    "    return dfCompiledEcho\n",
    "\n",
    "def get_merge_dfs(dfOutcome, dfEcho, dfEncounter):\n",
    "    dfExpandedOutcome = expand_outcome(dfOutcome)\n",
    "    dfCompiledEcho = compile_echo(dfEcho)\n",
    "    dfEncounter = dfEncounter.set_index('HSP_ENC')\n",
    "    \n",
    "    dfMerged = dfEncounter.merge(dfExpandedOutcome, how='left', on='HSP_ENC')\n",
    "    dfMerged = dfMerged.merge(dfCompiledEcho, how='left', on='HSP_ENC')\n",
    "    \n",
    "    return dfMerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preproc_labeled_echo(dfEchoLabeled):\n",
    "    # Changing mlid to mild\n",
    "    dfEchoLabeled['function'] = dfEchoLabeled['function'].replace('mlid', 'mild')\n",
    "    dfEchoLabeled['dilation'] = dfEchoLabeled['dilation'].replace('mlid', 'mild')\n",
    "    \n",
    "    # na to 0, mild to 1, moderate to 2, severe to 3\n",
    "    dictReplace = {'mild':1, 'moderate':2, 'severe':3}\n",
    "    dfEchoLabeled['function'] = dfEchoLabeled['function'].fillna(0)\n",
    "    dfEchoLabeled['function'] = dfEchoLabeled['function'].replace(dictReplace)\n",
    "\n",
    "    dfEchoLabeled['dilation'] = dfEchoLabeled['dilation'].fillna(0)\n",
    "    dfEchoLabeled['dilation'] = dfEchoLabeled['dilation'].replace(dictReplace)\n",
    "    \n",
    "    return dfEchoLabeled\n",
    "\n",
    "\n",
    "def merge_labeled_echo(dfMerged, dfEchoLabeled):\n",
    "    \"\"\"\n",
    "    Parse labeled echo data to keep matching enc, order_proc_id\n",
    "    If multiple rows per enc and order id, keep max bc this \n",
    "    represents different line of report\n",
    "    \"\"\"\n",
    "    srsIsNullNarrative = dfMerged['NARRATIVE_compiled'].isnull()\n",
    "    for nEnc in dfMerged.index:\n",
    "        nOrder = dfMerged.at[nEnc, 'ORDER_PROC_ID']\n",
    "        dfTemp = dfEchoLabeled[dfEchoLabeled['HSP_ENC']==nEnc]\n",
    "        dfTemp = dfTemp[dfTemp['ORDER_PROC_ID']==nOrder]\n",
    "        if dfTemp.shape[0] > 0:    \n",
    "            dfMerged.at[nEnc, 'echo_dilation'] = dfTemp['dilation'].max()\n",
    "            dfMerged.at[nEnc, 'echo_function'] = dfTemp['function'].max()\n",
    "        elif not srsIsNullNarrative[nEnc]:\n",
    "            dfMerged.at[nEnc, 'echo_dilation'] = 0\n",
    "            dfMerged.at[nEnc, 'echo_function'] = 0\n",
    "    \n",
    "    return dfMerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features to the verctors\n",
    "\n",
    "def vect_with_added_features(vect, X_train, X_test, df_features_to_add):\n",
    "    '''\n",
    "    add features to the vecterized X_train and X_test\n",
    "    '''\n",
    "    def add_feature(X, feature_to_add):\n",
    "        \"\"\"\n",
    "        Returns sparse feature matrix with added feature.\n",
    "        feature_to_add can also be a list of features.\n",
    "        \"\"\"\n",
    "        from scipy.sparse import csr_matrix, hstack\n",
    "        return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "    \n",
    "    if type(df_features_to_add) == pd.Series:\n",
    "        column_names = df_features_to_add.name\n",
    "    elif type(df_features_to_add) == pd.DataFrame:\n",
    "        column_names = df_features_to_add.columns\n",
    "        \n",
    "    \n",
    "    X_train_vected = vect.transform(X_train)\n",
    "    X_train_df = X_train.to_frame()\n",
    "    X_train_df = X_train_df.merge(df_features_to_add, how='left', left_index=True, right_index=True)\n",
    "    X_train_df = X_train_df.fillna(0)\n",
    "    X_train_added = add_feature(X_train_vected, np.array(X_train_df[column_names]).T)\n",
    "\n",
    "    \n",
    "    X_test_vected = vect.transform(X_test)\n",
    "    X_test_df = X_test.to_frame()\n",
    "    X_test_df = X_test_df.merge(df_features_to_add, how='left',  left_index=True, right_index=True)\n",
    "    X_test_df = X_test_df.fillna(0)\n",
    "    X_test_added = add_feature(X_test_vected, np.array(X_test_df[column_names]).T)\n",
    "    \n",
    "    return X_train_added, X_test_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features to the verctors\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(3,10)).fit(X_train)\n",
    "\n",
    "\n",
    "# new features\n",
    "X_copy = X.copy()\n",
    "X_copy['length'] = X_copy['CT_Echo'].map(len)\n",
    "X_copy['length1'] = X_copy['CT_Echo'].map(len)\n",
    "\n",
    "#df_new_features = X_copy[['length', 'length1']]\n",
    "df_new_features = df_new_features_echo_tags\n",
    "\n",
    "X_train_added, X_test_added = vect_with_added_features(vect, X_train, X_test, df_new_features)\n",
    "\n",
    "\n",
    "m = LogisticRegression().fit(X_train_added, y_train)\n",
    "y_scores_m = m.decision_function(X_test_added)\n",
    "y_proba_m = [i[1] for i in m.predict_proba(X_test_added)]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_m)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_m)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_echo(out_df, ech_df, enc_df, rad_df):\n",
    "    # read each dataset\n",
    "    mod_out_df = out_df.copy()\n",
    "    mod_out_df.reset_index(level=0, inplace=True)\n",
    "    mod_ech_df = ech_df.copy()\n",
    "    mod_ech_df.reset_index(level=0, inplace=True)\n",
    "    mod_enc_df = enc_df.copy()\n",
    "    mod_enc_df.reset_index(level=0, inplace=True)\n",
    "    mod_rad_df = rad_df.copy()\n",
    "    mod_rad_df.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    df_enc_echo_outcome = get_merge_dfs(mod_out_df, mod_ech_df, mod_enc_df)\n",
    "    df_enc_CT = merge_encounter_radiology(mod_enc_df, mod_rad_df)\n",
    "\n",
    "    # clean the dataset\n",
    "    df_cleaned = df_enc_echo_outcome.merge(df_enc_CT, how='left', on='HSP_ENC')\n",
    "\n",
    "    # raw column names\n",
    "    name_gen = []\n",
    "    for i in df_cleaned.columns.values:\n",
    "        if i.startswith('name_gen') or i.startswith('NARR'):\n",
    "            name_gen.append(i)\n",
    "    name_gen = sorted(name_gen) + ['b48hr']\n",
    "\n",
    "    # rename the column names\n",
    "    df_cleaned = df_cleaned[name_gen]\n",
    "    new_columns = ['CT', 'Echo', 'CPR', 'DEATH', 'INTUBATION', 'PPV', 'THROMBOLYSIS', 'THROMBOLYSIS_PROC', 'VASOPRESSORS', 'B48hr']\n",
    "    df_cleaned.columns = new_columns\n",
    "\n",
    "    # fill Nan\n",
    "    df_cleaned['CT'].fillna('No_CT', inplace=True)\n",
    "    df_cleaned['Echo'].fillna('No_Echo', inplace=True)\n",
    "    df_cleaned.fillna(0, inplace=True)\n",
    "\n",
    "    # convert to 1\n",
    "    map_dict = {'DEATH': 1, \n",
    "                'INTUBATION': 1,\n",
    "                'PPV': 1,\n",
    "                'THROMBOLYSIS': 1,\n",
    "                'THROMBOLYSIS_PROC': 1,\n",
    "                'VASOPRESSORS': 1,\n",
    "                0:0}\n",
    "    for col in ['DEATH', 'INTUBATION', 'PPV', 'THROMBOLYSIS', 'THROMBOLYSIS_PROC', 'VASOPRESSORS']:\n",
    "        df_cleaned[col] = df_cleaned[col].map(map_dict)\n",
    "        \n",
    "    \n",
    "    echo_tag_path = os.path.join(data_path, 'echo_tag.csv')\n",
    "    df_echo_label = pd.read_csv(echo_tag_path, encoding='ISO-8859-1')\n",
    "\n",
    "    dfEchoLabeled = preproc_labeled_echo(df_echo_label)\n",
    "    dfMerged = merge_labeled_echo(df_enc_echo_outcome, dfEchoLabeled)\n",
    "\n",
    "    #dfMerged.head()\n",
    "    df_new_features_echo_tags = dfMerged[['echo_dilation','echo_function']]\n",
    "    mergeRes = pd.merge(df_cleaned, df_new_features_echo_tags, on='HSP_ENC', how='left')\n",
    "        \n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unused rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(enc_df, data_path):\n",
    "    drop_path = os.path.join(data_path, \"final_enc_list.csv\")\n",
    "    drop_data = pd.read_csv(drop_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    mergeRes = pd.merge(enc_df, drop_data, on='HSP_ENC')\n",
    "    mergeRes.set_index(\"HSP_ENC\", inplace = True)\n",
    "    return mergeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Start calling merge functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = encounter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_echo(outcome_df, echo_df, encounter_df, radiology_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_cancer(output_data, cancer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_registry(output_data, registry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_vitals(output_data, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_co_morbid(output_data, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = merge_labs(output_data, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = output_data.pop('B48hr')\n",
    "output_data['B48hr']=outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = drop_rows(output_data, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(output_data.head(25).to_csv(\"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "## Preprocess / Normalize the merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = output_data.iloc[:, -1]\n",
    "X = output_data.copy()\n",
    "X = X.drop('B48hr', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "##  Define params and run gridsearch to find best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [5, 10, 15],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, None],\n",
    "    'min_samples_split': [2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring = 'roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
